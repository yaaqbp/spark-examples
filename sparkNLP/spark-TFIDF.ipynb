{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d630fe2-4091-4ebc-b153-b10e8ad3d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import IntegerType, ArrayType, StringType, DoubleType, MapType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1282f17d-470c-4295-a379-04bf874ab03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/18 12:50:33 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TFIDF\")\\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"25G\") \\\n",
    "    .config(\"spark.exceturor.memovyOverhead\", \"5G\") \\\n",
    "    .config(\"spark.executor.cores\", \"20\") \\\n",
    "    .config(\"spark.executor.cores.max\", \"20\") \\\n",
    "    .config(\"spark.driver.memory\", \"30G\") \\\n",
    "\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2dc045-e503-4a2f-8454-1ae7f37c0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dzone.com/articles/calculating-tf-idf-with-apache-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e2dd65-bf21-4a37-b636-105f62ebb26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.67 ms, sys: 694 Âµs, total: 8.36 ms\n",
      "Wall time: 4.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df = spark.read.parquet('/user/jpietraszek/arxiv-lemmatized_preprocessed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15dd0a17-81ef-4a33-ab9f-fa81d7e5e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.3 ms, sys: 9.44 ms, total: 72.8 ms\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Combine groupBy and explode in a single transformation\n",
    "df = df.groupBy(\"categories\").agg(f.explode(f.flatten(f.collect_list(\"result\"))).alias(\"token\"))\n",
    "\n",
    "# Create term frequency dataframe\n",
    "tokensWithTf = df.groupBy(\"categories\", \"token\").agg(f.count(\"token\").alias('tf'))\n",
    "\n",
    "# Create document frequency dataframe\n",
    "tokensWithDf = df.groupBy(\"token\").agg(f.countDistinct(\"categories\").alias('df'))\n",
    "\n",
    "# Calculate IDF using Spark SQL for better optimization\n",
    "tokensWithDf.createOrReplaceTempView(\"tokensWithDf\")\n",
    "tokensWithTf.createOrReplaceTempView(\"tokensWithTf\")\n",
    "\n",
    "docCount = spark.sql(\"SELECT COUNT(DISTINCT categories) FROM tokensWithTf\").collect()[0][0]\n",
    "tokensWithIdf = spark.sql(f\"SELECT *, LOG(({docCount} + 1.0) / (1.0 + df)) as idf FROM tokensWithDf\")\n",
    "\n",
    "# Join DataFrames\n",
    "joined_df = tokensWithTf.join(tokensWithIdf, [\"token\"], \"left\")\n",
    "\n",
    "# Calculate TF-IDF\n",
    "result_df = joined_df.withColumn(\"tf_idf\", f.col(\"tf\") * f.col(\"idf\"))\n",
    "\n",
    "# Create top 10 keywords per category based on TF-IDF value\n",
    "window_spec = Window.partitionBy(\"categories\").orderBy(f.col(\"tf_idf\").desc())\n",
    "ranked_df = result_df.withColumn(\"rank\", f.row_number().over(window_spec))\n",
    "top_tfidf_df = ranked_df.filter(f.col(\"rank\") <= 10)\n",
    "rank = top_tfidf_df.select('token', 'categories', 'rank')\n",
    "\n",
    "# Persist instead of cache for more control\n",
    "#rank.persist(StorageLevel.DISK_ONLY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81cab1ac-3602-42d7-a665-df122cd766dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:====================================================>   (42 + 3) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.7 ms, sys: 13.6 ms, total: 63.3 ms\n",
      "Wall time: 25.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Write the result to Parquet\n",
    "rank.write.format('parquet').mode(\"overwrite\").save(\"/user/jpietraszek/sparkTFIDF_full.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02e6fd1-7a25-42bf-9620-44da940061e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.25 ms, sys: 4.17 ms, total: 7.42 ms\n",
      "Wall time: 142 ms\n"
     ]
    }
   ],
   "source": [
    "# Code below has problems with lack of memory on executors, creates CODE 137 and several other problems. \n",
    "#This is caused by using UDF and more \"pandas\" code writing, mainly using aggregation and joins. Using SQL is much more efficient\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = spark.read.parquet('/user/jpietraszek/arxiv-lemmatized_preprocessed.parquet')%%time\n",
    "\n",
    "#df = df.sample(fraction = .001)\n",
    "df = df.groupBy(\"categories\").agg(f.flatten(f.collect_list(\"result\")).alias(\"merged_result\"))\n",
    "df = df.select(\"*\", f.explode(f.col(\"merged_result\")).alias(\"token\"))\n",
    "\n",
    "# create term frequency dataframe\n",
    "tokensWithTf = df.groupBy(\"categories\", \"token\").agg(f.count(\"merged_result\").alias('tf'))\n",
    "\n",
    "# create document frequency dataframe\n",
    "tokensWithDf = df.groupBy(\"token\").agg(f.countDistinct(\"merged_result\").alias('df'))\n",
    "def calc_idf(df, docCount):\n",
    "    return math.log((docCount + 1.0) / (1.0 + df))\n",
    "\n",
    "# register the IDF calculation function as a UDF\n",
    "calc_idf_udf = f.udf(calc_idf, DoubleType())\n",
    "docCount = tokensWithTf.select('categories').distinct().count()\n",
    "# apply the UDF to calculate IDF and add a new column \"idf\" on DF dataframe\n",
    "tokensWithIdf = tokensWithDf.withColumn(\"idf\", calc_idf_udf(f.col(\"df\"), f.lit(docCount)))\n",
    "\n",
    "joined_df = tokensWithTf.join(f.broadcast(tokensWithIdf), [\"token\"], \"left\")\n",
    "\n",
    "# calculate the TF-IDF by multiplying \"tf\" with \"idf\"\n",
    "result_df = joined_df.withColumn(\"tf_idf\", f.col(\"tf\") * f.col(\"idf\"))\n",
    "\n",
    "# create top 10 keywords per category based on TF-IDF value\n",
    "\n",
    "window_spec = Window.partitionBy(\"categories\").orderBy(f.col(\"tf_idf\").desc())\n",
    "ranked_df = result_df.withColumn(\"rank\", f.row_number().over(window_spec))\n",
    "top_tfidf_df = ranked_df.filter(f.col(\"rank\") <= 10)\n",
    "\n",
    "rank = top_tfidf_df.select('token','categories','rank')\n",
    "\n",
    "rank.write.format('parquet').mode(\"overwrite\").save(\"/user/jpietraszek/sparkTFIDF_full.parquet\")\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
